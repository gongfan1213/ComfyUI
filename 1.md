# COMP7065 Innovative Laboratory: Fine-tuning Stable Diffusion with LoRA
After completion of this lab, students will be able to:
1. Apply existing LoRAs to facilitate their own AI art design;
2. Describe how the LoRA fine-tuning custom node works;
3. Perform LoRA fine-tuning on stable diffusion models within ComfyUI;
4. Construct their own dataset for LoRA fine-tuning.

## Table of Contents
1. Apply existing LoRA: ‘Load LoRA’ node................................................................... 2 
2. Create your own LoRA: ‘SimpleLoRA’ node ............................................................ 5 
3. Exercise ................................................................................................................. 15

## 1. Apply existing LoRA: ‘Load LoRA’ node
LoRA stands for Low Rank Adapter. It is a parameter-efficient fine-tuning method for neural network models. To be more straightforward, LoRAs are small-sized ‘add on’ of model weights that allow you to efficiently customize the base stable diffusion model. For example, you can use LoRAs to change the image style, specify a character you like, etc. In this part, you will learn how to apply an existing LoRA to a stable diffusion model, and understand its effect in an intuitive manner. We will use the ‘80s Movie Style LoRA’ in this showcase. Please download the LoRA from the release website on CivitAI, and place its weight file ‘80s_offset.safetensors’ into the folder ‘ComfyUI/models/loras’.

![image](https://github.com/user-attachments/assets/835c5c8f-caf3-4002-a29f-a8e7dcca9b4d)


In ComfyUI, the ‘Load LoRA’ node provides us with a convenient way to utilize an existing LoRA. Below is showcase workflow that loads the ‘80s Movie Style LoRA’ and inserts it into the pruned Dreamshaper 8 model. As presented, the man in the output image looks just like a character from an old film.

![image](https://github.com/user-attachments/assets/3df4e762-8bed-46e2-afc7-73294a52c41b)


Now let’s investigate the settings of ‘Load LoRA’ node. The ‘lora_name’ specifies which LoRA should be loaded. The ‘strength_model’ and ‘strength_clip’ adjust the weight of LoRA applied to UNet and CLIP respectively. The range of two values is usually between 0 and 1. In the workflow, however, you are also allowed to set the two strengths to negative values so as to remove the effect from LoRA.

![image](https://github.com/user-attachments/assets/3d9ffe30-b256-4489-aa4a-a563e3cf1125)


Supplements: You should make sure that the LoRA matches the base model. For example, a LoRA for stable diffusion 1.5 models cannot be applied to a stable diffusion XL model. Additionally, LoRAs are usually trained with some trigger words. To make LoRA effective, you may need to input these trigger words into the positive textual prompts.
In general, you can find the information of trigger words and the expected base model from the release website of the LoRA. As shown below, the ‘80s Movie Style LoRA’ matches the ‘stable diffusion 1.5’ (SD1.5) model, and the trigger word is ‘80s’. Sometimes you may also find the values of ‘strength_model’ and ‘strength_clip’ recommended by the LoRA’s author, but you are always encouraged to try them by yourselves and find the most suitable ones.

![image](https://github.com/user-attachments/assets/038af4e5-fe3a-4ac0-8f60-dc3a704422b2)

![image](https://github.com/user-attachments/assets/707c3431-1f64-4ba3-ab93-c27f91fff769)



It’s also noteworthy that the base model in the showcase doesn’t have to be exactly the original Stable Diffusion 1.5. Other base models like Dreamshaper or DarkSushi are also supported, as long as they maintain the same network architecture with Stable Diffusion 1.5. Below is an example generated by DarkSushi 2.5D together with the ‘80’s Movie Style LoRA’.

![image](https://github.com/user-attachments/assets/14420348-c07e-44ab-8f49-449f0ec36eca)


To make sure that the LoRA indeed had an effect on the output, you can remove the ‘Load LoRA’ node, run the generation process again while keeping other settings unchanged. Below is an example generated without the LoRA. It shows that without LoRA the style of the figure greatly differs from the one given in showcase, and thereby we can conclude that the 80’s Movie Style LoRA is effective enough.

![image](https://github.com/user-attachments/assets/b350d55d-446a-4385-a40a-cab108a3e149)

## 2. Create your own LoRA: ‘SimpleLoRA’ node
Despite that there are thousands of LoRAs available in the AI art community (e.g. CivitAI), sometimes you may still be unable to find a LoRA that suits your art style. In this case you may need to LoRA fine-tune the base model by yourself. We will explore how to perform LoRA fine-tuning completely inside ComfyUI without any programming. This can be achieved by utilizing the SimpleLoRA node.
### Preparations
In this showcase, you will learn how to fine-tune the pruned Dreamshaper 8 model with a Huggingface dataset. We will use the ‘pokemon-blip-captions-en-zh’ dataset as the training set, and perform LoRA fine-tuning with the SimpleLoRA node.

Dataset: You can preview the ‘pokemon-blip-captions-en-zh’ dataset on HuggingFace through this link: https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-zh. In the showcase, the Pokemon dataset will be downloaded automatically, so it’s not necessary for you to manually download it from HuggingFace.

![image](https://github.com/user-attachments/assets/08e14f0e-b2fc-42eb-8f9a-d82501172225)



### SimpleLoRA
To enable LoRA fine-tuning in ComfyUI, we will need to utilize the custom node ‘SimpleLoRA’. You can access this node through this link on Github: https://github.com/sifengshang/ComfyUI_SimpleLoRA.
On the main website, I’ve explained how to install the node to your ComfyUI. You should first download the source codes of SimpleLoRA from the Github website, and place it into the folder for custom nodes: ‘ComfyUI\custom_nodes’.

![image](https://github.com/user-attachments/assets/02a8e8a6-5ab2-4ef9-89fd-5630396f9571)


Next, please close your ComfyUI window and terminal. You need to enter the folder ‘ComfyUI_SimpleLoRA’, and double click the batch file ‘install_requirements.bat’. It will automatically download and configure the Python dependencies of the node.

![image](https://github.com/user-attachments/assets/4d42e5bb-b003-4105-92e9-ef9dd49b0d1f)

To check whether the installation is successful, you can restart ComfyUI, right click and select ‘Add Node’. If you can see a selection of ‘SimpleLoRA’, then the custom node has been indeed successfully installed.

![image](https://github.com/user-attachments/assets/78e050d9-bbc2-4e2d-a668-25005e7072f2)



### Fine-tune Dreamshaper 8 with SimpleLoRA
To utilize the node for LoRA fine-tuning, you should first load a completely empty workflow in ComfyUI, then find the ‘SimpleLoRA’ node and add it to the workflow. The node can be found by: ‘Add Node’->’SimpleLoRA’->’LoRATrainer’ in turn. In other words, the workflow for LoRA fine-tuning with SimpleLoRA should contain only a single node (LoRATrainer) without any connections, which should look like this:

![image](https://github.com/user-attachments/assets/d09a248c-165d-4f1c-92c5-1feebc00b853)


```
#10 SimpleLoRA
LoRATrainer
ckpt_name DreamShaper_8_pruned.safetensors
use_custom_dataset false
dataset_path
lora_checkpointfolder loratest
lora_name my_lora
caption_column text
learning_rate 0.00010
batch_size 1
training_steps 500
checkpointing_steps 100
rank 4
random seed 0
```
This node enables you to LoRA fine-tune the base stable diffusion model without any coding! All you need to do is to configure the settings of the node properly, and then run the workflow by clicking the ‘Queue’ button.

The following part is a showcase teaching you how to LoRA fine-tune the pruned Dreamshaper 8 on ‘pokemon-blip-captions-en-zh’ dataset through the SimpleLoRA node. After applying the trained LoRA, the pokemon image generated by the base model will become more cartoon style. From this showcase you can learn the meaning of each setting in SimpleLoRA node and how to configure them reasonably based on your need.
#### ‘ckpt_name’
This setting indicates the ‘checkpoint name’ of the base model to be fine-tuned. It allows you to select a model from the folder ‘ComfyUI/models/checkpoints’. In this showcase our base model is the pruned Dreamshaper 8, so we can just set the ‘ckpt_name’ to ‘Dreamshaper_8_pruned.safetensors’.
#### ‘use_custom_dataset’ and ‘dataset_path’
The two settings are combined together to indicate which dataset will be used for LoRA fine-tuning. To be more specific, when set ‘use_custom_dataset’ to True, the node will load a custom dataset constructed by yourself, and the folder path of the dataset will be specified by the setting ‘dataset_path’. You will learn more details about how to build a custom dataset in the exercise. On the other hand, when setting ‘use_custom_dataset’ to False, the node will automatically download a HuggingFace dataset and use it for finetuning. The name of the dataset is specified by ‘dataset_path’.
In this showcase we will use a HuggingFace dataset ‘pokemon-blip-captions-en-zh’ as the training set. So we should set the ‘use_custom_dataset’ to False. As for the ‘dataset_path’, you can refer to the release website of ‘pokemon-blip-captions-en-zh’ dataset on HuggingFace: https://huggingface.co/datasets/svjack/pokemon-blip-captions-en-zh. On the website you can see the name of dataset is ‘svjack/pokemon-blip-captions-en-zh’, which is exactly what we should put for the ‘dataset_path’. You can also directly copy the dataset name to clipboard by clicking the copy icon (circled in blue) and then paste it to ‘dataset_path’.
Supplements: In general, any text-to-image dataset on HuggingFace can be supported.

![image](https://github.com/user-attachments/assets/ac1e9cd3-1638-49fa-b717-3d50c14ee097)


#### ‘lora_checkpoint_folder’ and ‘lora_name’
During the LoRA fine-tuning process, the node will save the trained LoRA weights periodically into the folder: ‘ComfyUI/models/lora_logs/[lora_checkpoint_folder]’. For example, if we set the ‘checkpoint_dirname’ to ‘lora_test’, then the trained LoRA weights will be saved into the folder: ‘ComfyUI/models/lora_logs/lora_test’. Note that, this folder will be automatically created by the node during training so you don’t have to create it by yourself manually. And the filename of the LoRA weights in ‘.safetensors’ format will be ‘[lora_name].safetensors’. For example, when setting ‘lora_name’ to ‘my_lora’, the filename will be ‘my_lora.safetensors’.
In this showcase, we can set the ‘lora_checkpoint_folder’ to ‘lora_showcase’, and ‘lora_name’ to ‘pokemon_lora’. As a result, a folder in ‘ComfyUI/models/lora_logs/lora_showcase’ will be created, and the trained lora weights will be named as ‘pokemon_lora.safetensors’.
#### ‘caption_column’
Before explaining the meaning of this setting, we should understand some basics about the dataset. We’ve learnt from the lecture that to train a neural network, one must prepare many training data organized as a dataset. In our showcase, you may imagine the whole dataset as a big Excel table, as shown in the illustration below. In this table, item in each row consists of a single image and at least one caption. The caption is exactly the positive textual prompts. And the base model is trained to generate exactly the content in the image when given the caption. Sometimes the training data may have multiple caption columns. As shown in the illustration below, for each image there are both English captions ‘en_text’ and Chinese captions ‘cn_text’. In this case we need to specify which caption column is used for training by specifying the ‘caption_column’ in the node.
It should be noted that there are different ways to set ‘caption_column’. When using a custom dataset (i.e., when setting custom_dataset to True), the key of ‘caption_column’ is always ‘text’. For an existing HuggingFace dataset, you may need to enter its release website and take a look at the columns in the Dataset Viewer. In our showcase, the Dataset Viewer of ‘pokemon-blip-captions-en-zh’ on the release website shows that there are two columns for captions. One is ‘en_text’ and the other is the ‘cn_text’. Since we use English captions for fine-tuning, so we set ‘caption_column’ to ‘en_text’.

![image](https://github.com/user-attachments/assets/b0be349f-bb04-4ce3-98b6-ba9b4beb6570)


#### ‘learning_rate’
This parameter controls exactly the learning rate during training. A larger learning rate speeds up the training, but it may also cause the loss decrease to be unstable or even lead to diverge. On the other hand, if the learning rate is too small, it may lead to underfitting even after many training steps. In this showcase, it is recommended to set the learning rate to its default value ‘0.0001’.
#### ‘batch_size’
When setting ‘batch_size’ to 1, only one pair of (image, caption) will be fed to the model for each training step. This may slow down the training process, but will significantly save the cost of GPU memory. If you have enough GPU resources, you can set the ‘batch_size’ larger than 1, as long as the ‘Cuda: out of memory’ error is not triggered. In the Lab we will use a single NVIDIA RTX 3050Ti with only 8GB GPU memory for fine-tuning, so it is recommended to set the ‘batch_size’ to 1.
#### ‘training_steps’ and ‘checkpointing_steps’
The ‘training_steps’ denotes the total iterations of the training process, and the LoRA weights will be saved every ‘checkpointing_steps’. For example, if we set the ‘training_steps’ to 2000 and ‘checkpointing_steps’ to 500, this means that the whole training steps will be 2000 and the LoRA weights will be saved every 500 steps.
It should be noted that LoRA trained with more ‘training_steps’ may be effective, but the training time will also increase proportionally. And if the training steps are too few, the the LoRA weights may be underfitting, resulting in bad quality. However, we cannot set the training steps to a very large value either, because the LoRA weights may suffer from overfitting, still resulting in bad output effect. In this showcase, in order to obtained a trained LoRA quickly, we can set the ‘training_steps’ to 1000 and ‘checkpoint_steps’ to 200. But you are also encouraged to set them by yourself and see how the quality of LoRA will be affected.
#### ‘rank’
This is a hyperparameter specifically for the LoRA fine-tuning method. It indicates how many ranks are there for the adapters in each layer. In general, higher ranks may improve the quality of LoRA, but will cost more GPU memory. In our showcase, it is recommended to set the ‘rank’ to 4.
#### ‘random_seed’
Similar to the random seed in the Ksampler, it is an integer number that controls the randomness during training. We can always set it to the default value 0.

Below is an illustration showing correct settings of LoRATrainer node in our Pokemon showcase. You may refer to it for a quick check.

![image](https://github.com/user-attachments/assets/0309e4cc-a42c-46b6-84a1-199030690b4e)


```
#10 SimpleLoRA
LoRATrainer
ckpt_name DreamShaper_8_pruned.safetensors
use_custom_dataset false
dataset_path svjack/pokemon-blip-captions-e
lora_checkpoint_folder lora_showcase
lora_name pokemon_lora
caption_column en_text
learning_rate 0.00010
batch_size
training_steps 1000
checkpointing_steps 200
rank 4
random_seed 0
```
After properly setting the LoRATrainer node, you may press the ‘Queue’ button to start the training workflow. The training process may take around 15 minutes. And you can supervise the training progress through the terminal window of ComfyUI.

![image](https://github.com/user-attachments/assets/4eeda429-0321-4197-864d-0ce07415ce31)


If the training is successful, you may see a message says “Training is finished” prompted in the terminal window.

![image](https://github.com/user-attachments/assets/7a5d3d01-e75a-42c3-9eaf-75b78626d4ea)


To make sure that the LoRA weights indeed have been successfully trained, you can check the folder ‘ComfyUI/models/lora_logs/lora_showcase’. As shown below, this folder indeed exists. From the generated files we can infer that LoRA weights are saved every 200 steps and are trained 1000 steps in total, consistent with our settings.

![image](https://github.com/user-attachments/assets/957fb892-f5f2-473e-b9e1-325eea95c43c)


Then we may also investigate the folder ‘ComfyUI/models/loras’, in which we see the trained weights ‘pokemon_lora.safetensors’. This further demonstrated that the LoRA training was successful.

![image](https://github.com/user-attachments/assets/dad1b1f9-f943-4bb9-9a14-d0b0f9ec15fa)


To directly show the effect of trained LoRA, we may build a workflow and apply the trained LoRA weights to the DreamShaper 8.0 model. The workflow is very similar to the showcase in Part 2. However, a major difference is that this time we should use the node ‘LoraLoadrModelOnly’ to replace ‘Load LoRA’ node, because in SimpleLoRA, the LoRA adapters are applied to transformer layers of UNet model only. You may refer to the illustration below for a reference. When putting the positive textual prompts as ‘a pokemon that looks like a mouse’, the model with the LoRA outputs an image that really looks like a Pokemon in the shape of a mouse.

![image](https://github.com/user-attachments/assets/7b204637-b19f-4c2e-9d3b-070ba3fcfc5f)


When we remove the node ‘LoraLoaderModelOnly’, the output image looks like a real mouse, instead of a Pokemon. This demonstrates that the LoRA effectively shifts the style of output image from realistic to Pokemon-like.

![image](https://github.com/user-attachments/assets/b742178d-3999-4344-b4d3-5db7c6417e8c)

